{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import necessary libraries"
      ],
      "metadata": {
        "id": "McMI6b-8fB1M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBFqKopAe-BB"
      },
      "outputs": [],
      "source": [
        "!pip install transformers supervision trackers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import supervision as sv\n",
        "from trackers import DeepSORTFeatureExtractor, DeepSORTTracker\n",
        "from transformers import AutoModelForObjectDetection, AutoImageProcessor"
      ],
      "metadata": {
        "id": "ZvBth9aefF3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining constants"
      ],
      "metadata": {
        "id": "WXWV5TckfMiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create a color palette for visualization\n",
        "# These hex color codes define different colors for tracking different objects\n",
        "color = sv.ColorPalette.from_hex([\n",
        "    \"#ffff00\", \"#ff9b00\", \"#ff8080\", \"#ff66b2\", \"#ff66ff\", \"#b266ff\",\n",
        "    \"#9999ff\", \"#3399ff\", \"#66ffff\", \"#33ff99\", \"#66ff66\", \"#99ff00\"\n",
        "])\n",
        "\n",
        "# Set the color lookup mode to assign colors by track ID\n",
        "# This mean objects with the same track ID will be annotated by the same color\n",
        "color_lookup = sv.ColorLookup.TRACK"
      ],
      "metadata": {
        "id": "vJYqBNClfI2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Demo Video"
      ],
      "metadata": {
        "id": "XqvwCPvYfTfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input and output video paths\n",
        "source_video_path = \"/content/Video_Ready_Cat_and_Play.mp4\"\n",
        "save_video_path = \"/content/DFine_Object_Detection_Result.mp4\"\n",
        "\n",
        "# Extract video information (width, height, fps) from the source\n",
        "video_info = sv.VideoInfo.from_video_path(source_video_path)\n",
        "print(video_info)"
      ],
      "metadata": {
        "id": "j_rNoEPkfI46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Detection Model"
      ],
      "metadata": {
        "id": "dlJXXuBYfZX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DFine model trained on Objects365 dataset\n",
        "checkpoint = \"ustc-community/dfine-large-obj365\"\n",
        "print(f\"Loading object detection model: {checkpoint}\")\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
        "model = AutoModelForObjectDetection.from_pretrained(checkpoint).to(device)\n",
        "\n",
        "label2id = {k.lower(): v for k, v in model.config.label2id.items()}"
      ],
      "metadata": {
        "id": "mRDMUxXgfI7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracking Model"
      ],
      "metadata": {
        "id": "BlENr5dHffAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the DeepSORT feature extractor with a MobileNetV4 backbone\n",
        "# it's not pretrained for ReID task, so you can find a better model on your own\n",
        "feature_extractor = DeepSORTFeatureExtractor.from_timm(\"mobilenetv4_conv_small.e1200_r224_in1k\")\n",
        "tracker = DeepSORTTracker(feature_extractor, frame_rate=video_info.fps)"
      ],
      "metadata": {
        "id": "UTuAACwmfI94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process Video"
      ],
      "metadata": {
        "id": "uLzG7KjSfkW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Box annotator draws rectangles around detected objects\n",
        "box_annotator = sv.BoxAnnotator(color, color_lookup=color_lookup)\n",
        "\n",
        "# Label annotator adds text labels to the detections: track id and class name\n",
        "label_annotator = sv.LabelAnnotator(color, color_lookup=color_lookup, text_color=sv.Color.BLACK, text_scale=0.8)"
      ],
      "metadata": {
        "id": "ZuZzqzgXfJAi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_CLASSES = [\"person\", \"cat\", \"other balls\"]"
      ],
      "metadata": {
        "id": "cNbcEAjefnEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_frame(frame, index):\n",
        "    \"\"\"\n",
        "    Process a single video frame: detect people, track them, and annotate the frame.\n",
        "\n",
        "    Args:\n",
        "        frame: The current video frame (numpy array)\n",
        "        index: The frame number in the sequence\n",
        "\n",
        "    Returns:\n",
        "        Annotated frame with detection boxes, labels, and traces\n",
        "    \"\"\"\n",
        "\n",
        "    inputs = image_processor(images=frame, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Convert raw model outputs to bounding boxes, labels, and scores\n",
        "    h, w, _ = frame.shape\n",
        "    detections = image_processor.post_process_object_detection(outputs, target_sizes=[(h, w)], threshold=0.3)\n",
        "    detections = detections[0]  # Get first image results (we're processing one frame at a time)\n",
        "\n",
        "    # Filter predictions by multiple classes instead of just \"person\"\n",
        "    if TARGET_CLASSES:\n",
        "        # Create a mask for all target classes\n",
        "        target_class_ids = [label2id[class_name.lower()] for class_name in TARGET_CLASSES if class_name.lower() in label2id]\n",
        "        keep = torch.isin(detections[\"labels\"], torch.tensor(target_class_ids).to(detections[\"labels\"].device))\n",
        "        detections = {k: v[keep] for k, v in detections.items()}\n",
        "\n",
        "    # Convert detections to Supervision format and update the tracker with new detections\n",
        "    detections = sv.Detections.from_transformers(detections, id2label=model.config.id2label)\n",
        "    detections = tracker.update(detections, frame=frame)\n",
        "\n",
        "    # Create labels for each detection\n",
        "    labels = [\n",
        "        f\"{model.config.id2label[class_id]}\"\n",
        "        for class_id, tracker_id\n",
        "        in zip(detections.class_id, detections.tracker_id)\n",
        "    ]\n",
        "\n",
        "    frame = box_annotator.annotate(scene=frame, detections=detections)\n",
        "    frame = label_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
        "\n",
        "    return frame"
      ],
      "metadata": {
        "id": "7XYCCKARfnG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sv.process_video(\n",
        "    source_path=source_video_path,\n",
        "    target_path=save_video_path,\n",
        "    callback=process_frame,  # Apply our processing function to each frame\n",
        "    show_progress=True,      # Display a progress bar\n",
        ")\n",
        "print(\"Video processing complete!\")"
      ],
      "metadata": {
        "id": "SrZ153lffJDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### View the result!"
      ],
      "metadata": {
        "id": "uXcooTOzfy9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to encode video with H264 codec to show in browser\n",
        "converted_video_path = save_video_path.replace(\".mp4\", \"-h264.mp4\")\n",
        "!ffmpeg -y -loglevel error -i {save_video_path} -vcodec libx264 -acodec aac {converted_video_path}"
      ],
      "metadata": {
        "id": "G8nwdKImfuQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Video\n",
        "Video(converted_video_path, embed=True, width=600)"
      ],
      "metadata": {
        "id": "Vx0WAS8cf2KL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}